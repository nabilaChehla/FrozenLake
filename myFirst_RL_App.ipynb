{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "33b3f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output  #You can use IPython.display.clear_output to clear the output of a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b8b46860",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode='ansi')\n",
    "#“ansi”: Return a strings (str) or StringIO.StringIO containing a terminal-style text representation for each time step. The text can include newlines and ANSI escape sequences (e.g. for colors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c412786e",
   "metadata": {},
   "source": [
    "# The Frooze Lake : \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./frozenLake.png\" alt=\"Frozen Lake\" style=\"max-width: 35%;\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d55f54",
   "metadata": {},
   "source": [
    "Creating our Q-table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "91328af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_size,action_size))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "faf4f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes =  10000         #we define the total number of episodes we want the agent to play during training\n",
    "max_steps_per_episode = 100  #we define the max number of steps the agent takes within an epesode, end of episode = HOLE, GOAL, or steps = 100\n",
    "\n",
    "\n",
    "learning_rate = 0.15\n",
    "discount_rate = 0.99\n",
    "\n",
    "# to implement epselon greedy policy\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# exploitation <-   0 < EPSELON < 1    -> exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b76bf",
   "metadata": {},
   "source": [
    "## Training : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dfeba0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []    # we save the reward we get after each episode (0 or 1)\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]   # init state to first one (where the agent in the beggining of the game is)\n",
    "    done = False\n",
    "    rewards_current_episode = 0 # init reward for that episode\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        #pick action : exploration  or explouitation ? \n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # Exploitation : choose the best action that gives max of reward for that state\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            # Exploration : randomly select one of the available actions.\n",
    "            action = env.action_space.sample() \n",
    "\n",
    "        # New state + Diagnostic : \n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward   # update reward \n",
    "        \n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration) by The exponontial decayed-epsilon-greedy strategy\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episode) \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "43b0b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.036000000000000025\n",
      "2000 :  0.20200000000000015\n",
      "3000 :  0.3930000000000003\n",
      "4000 :  0.5510000000000004\n",
      "5000 :  0.6300000000000004\n",
      "6000 :  0.6610000000000005\n",
      "7000 :  0.6720000000000005\n",
      "8000 :  0.6690000000000005\n",
      "9000 :  0.6770000000000005\n",
      "10000 :  0.7070000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ae2b27d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.52449898 0.4897745  0.49169598 0.48700615]\n",
      " [0.25963281 0.38987051 0.31448314 0.47556911]\n",
      " [0.37706937 0.41045103 0.41021191 0.44208053]\n",
      " [0.26070333 0.28273258 0.25408185 0.43023636]\n",
      " [0.5475035  0.45283134 0.40483908 0.47418671]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.14390421 0.12891361 0.23353356 0.12831921]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.46092634 0.38903496 0.50168196 0.58510379]\n",
      " [0.49757424 0.62860682 0.50329825 0.41879714]\n",
      " [0.55226451 0.34915357 0.25115489 0.29077742]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41382297 0.57192419 0.7587762  0.48156323]\n",
      " [0.75520883 0.86257971 0.77209303 0.76351852]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4784e3a",
   "metadata": {},
   "source": [
    "#### Now we can watch the agent play by playing the best action :\n",
    "Remember when we introduced Frozen Lake, part of the description noted that the agent won't always take the action that it chooses to take because, since the ice is slippery, even if we choose to go right, for example, we may slip and go up instead. So keep this in mind as you watch the agent play because you may see the chosen action show as right but then see the agent take a step up, for example. The slippery ice is the reason for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b62c4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(5)\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        print(env.render())\n",
    "        time.sleep(0.3)\n",
    "        # Choose action with highest Q-value for current state     \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, truncated, info = env.step(action)  \n",
    "        #If agent fell in hole or won\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106dd338",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
